---
title: 't-tests: comparing two means'
author: "Miguel Conde"
date: "5 de mayo de 2017"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      fig.align = "center")
```

# The one-sample $z$-test

Let's suppose we are considering some population that can be described by a normal distribution whose parameters are perfectly knwon to us: mean $\mu_0 = 67.5$ and standard deviation $\sigma_0 = 10$.


```{r}
mu_0    <- 67.5
sigma_0 <- 10
```

Consider now *another* population, also described by a normal distribution and whose standard deviation is perfectly known to us, $\sigma = 9.5$

We now take some sample from this second population, let's say we obtain:
```{r}
sample_data <- c(50, 60, 60, 64, 66, 
                 66 ,67, 69, 70, 74, 
                 76, 76, 77, 79, 79,
                 79, 81, 82, 82, 89)
```

And calculate some summary statistics:
```{r}
mean(sample_data)
sd(sample_data)
```


We wonder if the mean of the second population is or not equal to the first's or it's been just a matter of chance (due to the small sample size) that $\bar{X} > \mu$.

This is rarelly a real situation because, in the real world, we probably won't be aware of the true distribution of the population (i.e., \sigma will be unknown).

Hence, our **null hypothesis** is:
$$
H_0: \mu = \mu_0 = 67.5
$$

And the **alternative hypothesis**:
$$
H_a: \mu \neq \mu_0 
$$


How can we discriminate between $H_0$ and $H_a$?

First, let's measure the distance between $\mu_0$ and $\bar{X}$:

$$
\bar{X} - \mu_0
$$

Is this quantity is pretty close to 0, the null hypothesis is quite likely to be true. But what does "pretty close to 0" mean?

If the null hypothesis is true then:

1 - We are certain that the second population is normal with mean $\mu_0$ and standar deviation $\sigma_1$.

$$
X \sim Normal(\mu_0, \sigma^2 )
$$

2 - By means of the CLT, the distribution of the sample means $\bar{X}$ is also normal with standard deviation (aka *standard error*):
$$
SE(\bar{X}) = \frac{\sigma}{\sqrt{N}}
$$

$$
\bar{X} \sim N(\mu_0, SE(\bar{X}))
$$

Well, let's measure now the same distance but in "number of standar errors" units.This is called a **standard score** (in this case, a $z$-score):

$$
z_{\bar{X}} = \frac{\bar{X} - \mu_0}{SE(\bar{X})} = \frac{\bar{X} - \mu_0}{\frac{\sigma}{\sqrt{N}}}
$$
Carefully notice that this $z$-score has a *standard normal distribution*:

$$
z_{\bar{X}} \sim Normal(0, 1)
$$

In other words, regardless of what scale the original data are on, the $z$-statistic itself always has the same interpretation: *it’s equal to the number of standard errors that separate the observed sample mean $\bar{X}$ from the population mean $\mu_0$ predicted by the null hypothesis*.

And the $\alpha$% critical regions for $z$-test are always the same, as illustrated in this table:

| desired $\alpha$ level | 2-sided test | 1-sided                           |
|:----------------------:|:------------:|:---------------------------------:|
| .1                     | `r -qnorm(.1/2)`  | `r qnorm(0.1, lower.tail = FALSE)`    |
| .05                    | `r -qnorm(.05/2)`  | `r qnorm(.05, lower.tail = FALSE)`   |
| .01                    | `r -qnorm(.01/2)`  | `r qnorm(.01, lower.tail = FALSE)`   |
| .001                   | `r -qnorm(.001/2)`  | `r qnorm(.001, lower.tail = FALSE)` |

## Example
### Step by step
```{r}
mu_0  <- 67.5
sigma <- 9.5

N <- length(sample_data)

sem <- sigma / sqrt(N) # Standard error = standard deviation of the mean

sample_mean <- mean(sample_data)

z_score <- (sample_mean - mu_0) / sem
z_score
```
Let's calculate the p-value.

```{r}
upper_area <- pnorm(q = z_score, lower.tail = FALSE)
upper_area

lower_area <- pnorm(q = -z_score)
lower_area
```

```{r}
p_value <- upper_area + lower_area
p_value
```

### With R
Study carefully `z_test()`
```{r}
source("z_Test.R")
z_test(x = sample_data , mu = mu_0, stdev = sigma)
```

We could sum up this way:

*"With a mean of `r sample_mean` in the sample , and assuming a true population standard deviation of `r sigma`, we can conclude that the sample population mean have significantly different statistics scores to the reference population ($z = `r round(z_score, 3)`$, $N = `r N`$, $p = `r round(p_value, 3)`$)."*

### Assumptions of the $z$-test

1. *Normality*. The z-test assumes that the true population distribution is normal.
2. *Independence*. The observations in your data set are not correlated with each other, or related to each other in some way. You have to ask yourself if it’s really plausible to imagine that each observation is a completely random sample from the population that you’re interested in.
3. *Known standard deviation*. The true standard deviation
of the population is known to the researcher. This is just stupid. In no real world data analysis problem do you know the standard deviation $\sigma$ of some population, but are completely ignorant about the mean $\mu$. In other words, this assumption is *always wrong*.

In view of the third asumption, we must move on to a more realistic test, the *t-test*.

# The one-sample t-test
Coming back to our example, all we have is our raw data that can give me an *estimate* of the population standard deviation:
```{r}
sd(sample_data)
```

I.e., while we can't say that $\sigma = 9.5$, we *can* say that $\hat{\sigma} = `r round(sd(sample_data),2)`$

Perhaps we could run a $z$-test using $\hat{\sigma} = `r round(sd(sample_data),2)`$ instead of $\sigma = 9.5$. In fact this approach could by chance throw a significant result, but not *quite* correct.

The Gosset's "Student test" solves this problem in a very similar manner as the $z$-test does. 

The null hypothesis is that the population mean $\mu$ is equal to some specified value $\mu_0$, and the alternative hypothesis is that it is not. Like the $z$-test, we assume that the data are normally distributed; but we do not assume that the population standard deviation $\sigma$ is known in advance.

The key thing that Gosset figured out is how we should accommodate the
fact that we aren’t completely sure what the true standard deviation is. The answer is that it subtly changes the sampling distribution. In the t-test, our test statistic (now called a t-statistic) is calculated in exactly the same way I mentioned above.

$$
t =\frac{\bar{X} - \mu_0}{\hat{\sigma}\sqrt(N)}
$$

The only thing that has changed in the equation is that instead of using the known true value $\sigma$, we use the estimate $\hat{\sigma}$.

And if this estimate has been constructed from $N$ observations, then the sampling distribution turns into a t-distribution with $N-1$ **degrees of freedom** (df). 

The t distribution is very similar to the normal distribution, but has “heavier” tails. Notice, though, that as df gets larger, the t-distribution starts to look identical to the standard normal distribution.

```{r}
N <- 11
x_seq <- seq(-4, 4, length.out = N)

n_t_d <- data.frame(x = x_seq, 
                    n_distr = dnorm(x_seq), 
                    t_distr = dt(x_seq, df = N - 1))
library(highcharter)
hc <- highchart() %>% 
  hc_add_series(name = "N(0,1)", type = "spline", data = n_t_d, 
                hcaes(x = x, y = n_distr)) %>%
  hc_add_series(name = "t(10)", type = "spline", data = n_t_d, 
                hcaes(x = x, y = t_distr)) %>%
  hc_title(text = "Normal(0, 1) vs. t distribution (df = 10)")
hc
```

## t-test in R

Doing a t-test step by step is almst the same as a $z$-test, so we won't do it.

In practice we'll use `t.test()`.

```{r}
t.test(x = sample_data,
       mu = 67.5,        # the population mean is 67.5 if the null is true
       alternative = "two.sided", # default
       conf.level = .95  # default
       )
```

We could report the result by saying something like this:
```{r include = FALSE}
t_test<- t.test(x = sample_data,
       mu = 67.5,        # the population mean is 67.5 if the null is true
       alternative = "two.sided", # default
       conf.level = .95  # default
       )
```


With a mean grade of $`r mean(sample_data)`$, the sample data shows a slightly higher mean than the average of $`r mu_0`$ ($t(`r t_test$parameter`) = `r as.numeric(round(t_test$statistic, 2))`$, $p < 0.05$); the 95% confidence interval is $[`r round(t_test$conf.int[1], 1)`, `r round(t_test$conf.int[2], 1)`]$.

## Assumptions of the t-test

1. *Normality*. We’re still assuming that the phe population distribution is normal.
2. *Independence*. Once again, we have to assume that the observations in our sample are generated independently of one another.

Overall, these two assumptions aren’t terribly unreasonable, and as a consequence the one-sample t-test is pretty widely used in practice as a way of comparing a sample mean against a hypothesised population mean.

# The 2 samples t-tests

## The independent samples t-test 

Although the one sample t-test has its uses, it’s not the most typical example of a t-test. 

A much more common situation arises when you’ve got two different groups of observations and the research question that you’re asking is whether or not the two groups have the same population mean (i.e., if the difference between the two sample means is zero). 

$$
H_0: \\
H_1:
$$
This is the situation that the independent samples t-tests are designed for.


### Both populations have the same variance - Student test 




### Populations with different variances - Welch test
One of the biggest problems with the Student test is the assumption that both groups have exactly the same variance, which is often not satisfied by real data.

It’s a bit odd to talk about the cure before talking about the diagnosis, but as it happens the Welch test is the default t-test in R, so this is probably the best place to discuss it.



## The paired-samples t-test
Regardless of whether we’re talking about the Student test or the Welch test, an independent samples t-test is intended to be used in a situation where you have two samples that are, well, independent of one another. 

This situation arises naturally when participants are assigned randomly to one of two experimental conditions, but it provides a very poor approximation to other sorts of research designs. 

In particular, a repeated measures design – in which each participant is measured (with respect to the same outcome variable) in both experimental conditions – is not suited for analysis using independent samples t-tests. 

For example, we might be interested in whether listening to music reduces people’s working memory capacity. To that end, we could measure each person’s working memory capacity in two conditions: with music, and without music. In an experimental design such as this one, each participant appears in both groups.

This requires us to approach the problem in a different way; by using the **paired samples t-test**.

In short, this 2 paired samples test is identical to a 1 sample test where the sample data is the *difference* of the 2 samples data

$$
H_0: \\
H_1:
$$

# Effect size

# Checking the normality of a sample

## QQ plots

## Shapiro-Wilk tests

# Testing non-normal data with Wilcoxon tests

## Two sample Wilcoxon test

## One sample Wilcoxon test

